{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG6fwukq5oSe"
      },
      "source": [
        "# Load your data\n",
        "\n",
        "Before finetuning a pretrained model of the experiments we provide in our repository (or precomputed and provided [here](https://datacloud.hhi.fraunhofer.de/nextcloud/s/NCjYws3mamLrkKq)), first load your custom 100 Hz sampled 12-lead ECG signal data `X` of shape `[N,L,12]` in Millivolts (mV) and multi-hot encoded labels `y` of shape `[N,C]` as numpy arrays, where `C` is the number of classes and `N` the number of total samples in this dataset. Although PTB-XL comes with fixed `L=1000` (i,e. 10 seconds), it is not required to be fixed, **BUT** the shortest sample must be longer than `input_size` of the specific model (e.g. 2.5 seconds for our fastai-models).\n",
        "\n",
        "For proper tinetuning split your data into four numpy arrays: `X_train`,`y_train`,`X_val` and `y_val`\n",
        "\n",
        "### Example: finetune model trained on all (71) on superdiagnostic (5)\n",
        "Below we provide an example for loading [PTB-XL](https://physionet.org/content/ptb-xl/1.0.1/) aggregated at the `superdiagnostic` level, where we use the provided folds for train-validation-split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0L-abXib6HkK",
        "outputId": "472d330c-0510-416a-cfe0-a545e80d38c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wfdb\n",
            "  Downloading wfdb-4.1.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib<4.0.0,>=3.2.2 in /usr/local/lib/python3.8/dist-packages (from wfdb) (3.2.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wfdb) (1.7.3)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.10.1 in /usr/local/lib/python3.8/dist-packages (from wfdb) (1.21.6)\n",
            "Requirement already satisfied: pandas<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wfdb) (1.3.5)\n",
            "Requirement already satisfied: SoundFile<0.12.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from wfdb) (0.11.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from wfdb) (2.25.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas<2.0.0,>=1.0.0->wfdb) (2022.7.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.8.1->wfdb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.8.1->wfdb) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.8.1->wfdb) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.8.1->wfdb) (1.24.3)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.8/dist-packages (from SoundFile<0.12.0,>=0.10.0->wfdb) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0->SoundFile<0.12.0,>=0.10.0->wfdb) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib<4.0.0,>=3.2.2->wfdb) (1.15.0)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9674 sha256=09ec8b04d514d3cf890037f4569ee419b94f145c81bb1a85b7cffa31ae0205b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/a8/c3/3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, tensorflow-addons, wfdb\n",
            "Successfully installed tensorflow-addons-0.19.0 wfdb-4.1.0 wget-3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install wget wfdb tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "Y4NQeLDBZUlm",
        "outputId": "597b0155-5c98-4d66-d50c-fb8f8ed81794",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Jqxss_rS6Jxn"
      },
      "outputs": [],
      "source": [
        "import wget\n",
        "import numpy as np\n",
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pickle\n",
        "import tensorflow_addons as tfa\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcjcSuO06o-6",
        "outputId": "f35858d8-1bb1-4e24-d948-e301f0d56732"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-09 18:21:51--  https://physionet.org/static/published-projects/ptb-xl/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1.zip\n",
            "Resolving physionet.org (physionet.org)... 18.18.42.54\n",
            "Connecting to physionet.org (physionet.org)|18.18.42.54|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1842722380 (1.7G) [application/zip]\n",
            "Saving to: ‘ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1.zip’\n",
            "\n",
            "b-xl-a-large-public  28%[====>               ] 495.66M  2.07MB/s    eta 12m 40s"
          ]
        }
      ],
      "source": [
        "!wget https://physionet.org/static/published-projects/ptb-xl/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1.zip\n",
        "\n",
        "os.mkdir(\"./data/\")\n",
        "\n",
        "\n",
        "with zipfile.ZipFile(\"./ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"./data/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Qv9hVm-5wyi"
      },
      "outputs": [],
      "source": [
        "!pip install GitPython\n",
        "from git import Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4T-c5KI5y2W"
      },
      "outputs": [],
      "source": [
        "HTTPS_REMOTE_URL = 'https://github.com/Bsingstad/ecg_ptbxl_benchmarking.git'\n",
        "DEST_NAME = 'github_repo'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8hm-j6751kC"
      },
      "outputs": [],
      "source": [
        "Repo.clone_from(HTTPS_REMOTE_URL, DEST_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRUZICm6562J"
      },
      "outputs": [],
      "source": [
        "from github_repo.code import *\n",
        "%matplotlib inline\n",
        "%load_ext autoreload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmW7xaR_5oSf"
      },
      "outputs": [],
      "source": [
        "from github_repo.code.utils import utils\n",
        "\n",
        "sampling_frequency=100\n",
        "datafolder='./data/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/'\n",
        "task='all'\n",
        "outputfolder='./github_repo/output/'\n",
        "\n",
        "\n",
        "# Load PTB-XL data\n",
        "data, raw_labels = utils.load_dataset(datafolder, sampling_frequency)\n",
        "# Preprocess label data\n",
        "labels = utils.compute_label_aggregations(raw_labels, datafolder, task)\n",
        "# Select relevant data and convert to one-hot\n",
        "#data, labels, Y, _ = utils.select_data(data, labels, task, min_samples=0, outputfolder=outputfolder)\n",
        "\n",
        "data, labels, Y, _ = utils.select_data(data, labels, task, min_samples=0, outputfolder=outputfolder)\n",
        "\n",
        "# 1-9 for training \n",
        "X_train = data[labels.strat_fold < 10]\n",
        "y_train = Y[labels.strat_fold < 10]\n",
        "# 10 for validation\n",
        "X_val = data[labels.strat_fold == 10]\n",
        "y_val = Y[labels.strat_fold == 10]\n",
        "\n",
        "num_classes = 71         # <=== number of classes in the finetuning dataset\n",
        "input_shape = [1000,12] # <=== shape of samples, [None, 12] in case of different lengths\n",
        "\n",
        "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.bar(np.unique(y_train.argmax(axis=1),return_counts=True)[0],np.unique(y_train.argmax(axis=1),return_counts=True)[1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DIKpffEJMllK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "_, X_grid, _, y_grid= train_test_split(X_train, y_train, test_size=0.10, random_state=42)"
      ],
      "metadata": {
        "id": "-a6LeAwqOm_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_grid.shape"
      ],
      "metadata": {
        "id": "QW5xSWezMWo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(np.unique(y_grid.argmax(axis=1),return_counts=True)[0],np.unique(y_grid.argmax(axis=1),return_counts=True)[1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fDz352LpQNds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UyIF9kv5oSg"
      },
      "source": [
        "# Train or download models\n",
        "There are two possibilities:\n",
        "   1. Run the experiments as described in README. Afterwards you find trained in models in `output/expX/models/`\n",
        "   2. Download the precomputed `output`-folder with all experiments and models from [here]((https://datacloud.hhi.fraunhofer.de/nextcloud/s/NCjYws3mamLrkKq))\n",
        "\n",
        "# Load pretrained model\n",
        "\n",
        "For loading a pretrained model:\n",
        "   1. specify `modelname` which can be seen in `code/configs/` (e.g. `modelname='fastai_xresnet1d101'`)\n",
        "   2. provide `experiment` to build the path `pretrainedfolder` (here: `exp0` refers to the experiment with `all` 71 SCP-statements)\n",
        "   \n",
        "This returns the pretrained model where the classification is replaced by a random initialized head with the same number of outputs as the number of classes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from github_repo.code.models.base_model import ClassificationModel\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "        \n",
        "class inception_time_model(ClassificationModel):\n",
        "    def __init__(self, name, n_classes,  sampling_frequency, outputfolder, input_shape, epoch=30, batch_size=32, lr_init = 0.001, lr_red=\"yes\", model_depth=6, loss=\"bce\", kernel_size=40, bottleneck_size=32, nb_filters=32, clf=\"binary\", verbose=0):\n",
        "        super(inception_time_model, self).__init__()\n",
        "        self.name = name\n",
        "        self.n_classes = n_classes\n",
        "        self.sampling_frequency = sampling_frequency\n",
        "        self.outputfolder = outputfolder\n",
        "        self.input_shape = input_shape\n",
        "        if loss == \"bce\":\n",
        "          self.loss = tf.keras.losses.BinaryCrossentropy()\n",
        "        elif loss == \"wbce\":\n",
        "          self.loss = tfa.losses.SigmoidFocalCrossEntropy() #focal instead of weighted bce\n",
        "        self.model = build_model((self.sampling_frequency*10,12),self.n_classes,lr_init = lr_init, depth=model_depth, kernel_size=kernel_size, bottleneck_size=bottleneck_size, nb_filters=nb_filters,clf=clf, loss=self.loss)\n",
        "        self.epoch = epoch\n",
        "        self.batch_size = batch_size\n",
        "        self.lr_red = lr_red\n",
        "        self.verbose = verbose\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "    def fit(self, X_train, y_train, X_val, y_val):\n",
        "\n",
        "        if self.lr_red == \"no\":\n",
        "            self.model.fit(X_train, y_train, epochs=self.epoch, batch_size=self.batch_size, \n",
        "            validation_data=(X_val, y_val), verbose=self.verbose)\n",
        "        elif self.lr_red == \"yes\":\n",
        "            self.model.fit(X_train, y_train, epochs=self.epoch, batch_size=self.batch_size, \n",
        "            validation_data=(X_val, y_val), verbose=self.verbose,\n",
        "            callbacks = [tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=0)])\n",
        "        else:\n",
        "            print(\"Error: wrong lr_red argument\")\n",
        "\n",
        "        #self.model.save(self.outputfolder +'last_model.h5')\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "\n",
        "def _inception_module(input_tensor, stride=1, activation='linear', use_bottleneck=True, kernel_size=40, bottleneck_size=32, nb_filters=32):\n",
        "\n",
        "    if use_bottleneck and int(input_tensor.shape[-1]) > 1:\n",
        "        input_inception = tf.keras.layers.Conv1D(filters=bottleneck_size, kernel_size=1,\n",
        "                                              padding='same', activation=activation, use_bias=False)(input_tensor)\n",
        "    else:\n",
        "        input_inception = input_tensor\n",
        "\n",
        "    # kernel_size_s = [3, 5, 8, 11, 17]\n",
        "    kernel_size_s = [kernel_size // (2 ** i) for i in range(3)]\n",
        "\n",
        "    conv_list = []\n",
        "\n",
        "    for i in range(len(kernel_size_s)):\n",
        "        conv_list.append(tf.keras.layers.Conv1D(filters=nb_filters, kernel_size=kernel_size_s[i],\n",
        "                                              strides=stride, padding='same', activation=activation, use_bias=False)(\n",
        "            input_inception))\n",
        "\n",
        "    max_pool_1 = tf.keras.layers.MaxPool1D(pool_size=3, strides=stride, padding='same')(input_tensor)\n",
        "\n",
        "    conv_6 = tf.keras.layers.Conv1D(filters=nb_filters, kernel_size=1,\n",
        "                                  padding='same', activation=activation, use_bias=False)(max_pool_1)\n",
        "\n",
        "    conv_list.append(conv_6)\n",
        "\n",
        "    x = tf.keras.layers.Concatenate(axis=2)(conv_list)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(activation='relu')(x)\n",
        "    return x\n",
        "\n",
        "def _shortcut_layer(input_tensor, out_tensor):\n",
        "    shortcut_y = tf.keras.layers.Conv1D(filters=int(out_tensor.shape[-1]), kernel_size=1,\n",
        "                                      padding='same', use_bias=False)(input_tensor)\n",
        "    shortcut_y = tf.keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "    x = tf.keras.layers.Add()([shortcut_y, out_tensor])\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def build_model(input_shape, nb_classes, depth=6, use_residual=True, lr_init = 0.001, kernel_size=40, bottleneck_size=32, nb_filters=32, clf=\"binary\", loss=tf.keras.losses.BinaryCrossentropy()):\n",
        "    input_layer = tf.keras.layers.Input(input_shape)\n",
        "\n",
        "    x = input_layer\n",
        "    input_res = input_layer\n",
        "\n",
        "    for d in range(depth):\n",
        "\n",
        "        x = _inception_module(x,kernel_size = kernel_size, bottleneck_size=bottleneck_size, nb_filters=nb_filters)\n",
        "\n",
        "        if use_residual and d % 3 == 2:\n",
        "            x = _shortcut_layer(input_res, x)\n",
        "            input_res = x\n",
        "\n",
        "    gap_layer = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    output_layer = tf.keras.layers.Dense(units=nb_classes,activation='sigmoid')(gap_layer)  \n",
        "    model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(loss=loss,\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=lr_init), \n",
        "                  metrics=[tf.keras.metrics.BinaryAccuracy(),\n",
        "                          tf.keras.metrics.AUC(\n",
        "                        num_thresholds=200,\n",
        "                        curve='ROC',\n",
        "                        summation_method='interpolation',\n",
        "                        name=\"ROC\",\n",
        "                        multi_label=True,\n",
        "                        ),\n",
        "                      tf.keras.metrics.AUC(\n",
        "                        num_thresholds=200,\n",
        "                        curve='PR',\n",
        "                        summation_method='interpolation',\n",
        "                        name=\"PRC\",\n",
        "                        multi_label=True,\n",
        "                        )\n",
        "              ])\n",
        "    print(\"Inception model built.\")\n",
        "    return model\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch % 5 == 0:\n",
        "        return lr*0.1\n",
        "    else:\n",
        "        return lr\n"
      ],
      "metadata": {
        "id": "kg7-fD1ocLBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEx-BdLJ5oSh"
      },
      "outputs": [],
      "source": [
        "#from github_repo.code.models.your_model import inception_time_model\n",
        "\n",
        "# change first line in your_model.py to from github_repo.code.models.base_model import ClassificationModel\n",
        "\n",
        "experiment = 'exp0'\n",
        "modelname = 'fastai_xresnet1d101'\n",
        "pretrainedfolder = '../output/'+experiment+'/models/'+modelname+'/'\n",
        "mpath='../output/' # <=== path where the finetuned model will be stored\n",
        "n_classes_pretrained = 71 # <=== because we load the model from exp0, this should be fixed because this depends the experiment\n",
        "\n",
        "model = inception_time_model(\"tf_inception\", num_classes, sampling_frequency, mpath, input_shape)\n",
        "\n",
        "#model = fastai_model(\n",
        "#    modelname, \n",
        "#    num_classes, \n",
        "#    sampling_frequency, \n",
        "#    mpath, \n",
        "#    input_shape=input_shape, \n",
        "#    pretrainedfolder=pretrainedfolder,\n",
        "#    n_classes_pretrained=n_classes_pretrained, \n",
        "#    pretrained=True,\n",
        "#    epochs_finetuning=2,\n",
        "#)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6dtMWU15oSh"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Preprocess data with pretrained Standardizer\n",
        "\n",
        "Since we standardize inputs to zero mean and unit variance, your custom data needs to be standardized with the respective mean and variance. This is also provided in the respective experiment folder `output/expX/data/standard_scaler.pkl`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqSSsK_V5oSh"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from github_repo.code.utils import utils\n",
        "\n",
        "#standard_scaler = pickle.load(open('./github_repo/output/'+experiment+'/data/standard_scaler.pkl', \"rb\"))\n",
        "\n",
        "#X_grid = utils.apply_standardizer(X_grid, standard_scaler)\n",
        "#X_val = utils.apply_standardizer(X_val, standard_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
        "\n",
        "def apply_standardizer(X, ss):\n",
        "    X_tmp = []\n",
        "    for x in X:\n",
        "        x_shape = x.shape\n",
        "        X_tmp.append(ss.transform(x.flatten()[:,np.newaxis]).reshape(x_shape))\n",
        "    X_tmp = np.array(X_tmp)\n",
        "    return X_tmp\n",
        "\n",
        "def preprocess_signals(X_train, X_validation):\n",
        "    # Standardize data such that mean 0 and variance 1\n",
        "    ss = StandardScaler()\n",
        "    ss.fit(np.vstack(X_train).flatten()[:,np.newaxis].astype(float))\n",
        "  \n",
        "    return apply_standardizer(X_train, ss), apply_standardizer(X_validation, ss)\n"
      ],
      "metadata": {
        "id": "lOSQURgFUycV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfdlaRZr5oSh"
      },
      "source": [
        "# Finetune model\n",
        "\n",
        "Calling `model.fit` of a model with `pretrained=True` will perform finetuning as proposed in our work i.e. **gradual unfreezing and discriminative learning rates**. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxzCOgaP5oSi",
        "outputId": "4ec349e4-6aee-4c73-9bc4-bb307d5c3e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running gridsearch nr 0 of 972...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inception model built.\n",
            "21/21 [==============================] - 1s 17ms/step\n",
            "Score CV 1 :    macro_auc\n",
            "0   0.781634 (AUROC score)\n",
            "Inception model built.\n",
            "21/21 [==============================] - 1s 17ms/step\n",
            "Score CV 2 :    macro_auc\n",
            "0   0.786307 (AUROC score)\n",
            "Inception model built.\n"
          ]
        }
      ],
      "source": [
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "df_grid = pd.read_csv(\"/content/drive/MyDrive/ptb-xl-gridsearch/gridsearch_params.csv\")\n",
        "df_score = pd.read_csv(\"/content/drive/MyDrive/ptb-xl-gridsearch/gridsearch_score_all.csv\")\n",
        "gs_left = df_grid.shape[0] - df_score.shape[0] # run only the remaining gridsearch\n",
        "init_rows = df_score.shape[0]\n",
        "\n",
        "for gsr in range(gs_left):\n",
        "  gsr = gsr + init_rows #if previous gridsearch results exists\n",
        "  print(\"Running gridsearch nr {} of {}...\".format(gsr,df_grid.shape[0]))\n",
        "  auc_score = []\n",
        "  for i, (train_index, test_index) in enumerate(skf.split(X_grid, y_grid.argmax(axis=1))):\n",
        "    model = inception_time_model(name=\"tf_inception\", n_classes=num_classes, sampling_frequency=sampling_frequency, outputfolder=mpath, input_shape=input_shape, \n",
        "                              epoch=int(df_grid.iloc[gsr][\"epochs\"]), batch_size=int(df_grid.iloc[gsr][\"batch_size\"]), lr_init = df_grid.iloc[gsr][\"init_lr\"],\n",
        "                              lr_red=df_grid.iloc[gsr][\"lr_red\"], model_depth=int(df_grid.iloc[gsr][\"model_depth\"]), loss=df_grid.iloc[gsr][\"loss\"], \n",
        "                              kernel_size=int(df_grid.iloc[gsr][\"kernel_size\"].split(\",\")[0].split(\"(\")[1]))\n",
        "    cv_x_train = X_grid[train_index]\n",
        "    cv_y_train = y_grid[train_index] \n",
        "    cv_x_test = X_grid[test_index]\n",
        "    cv_y_test = y_grid[test_index]\n",
        "    cv_x_train,cv_x_test = preprocess_signals(cv_x_train,cv_x_test)\n",
        "\n",
        "    model.fit(cv_x_train, cv_y_train, cv_x_test, cv_y_test)\n",
        "    y_hat_test = model.predict(cv_x_test)\n",
        "    score=utils.evaluate_experiment(np.vstack([cv_y_test,np.ones(cv_y_test.shape[1])]), np.vstack([y_hat_test,np.ones(y_hat_test.shape[1])]))\n",
        "    auc_score.append(score)\n",
        "    print(\"Score CV {} : {} (AUROC score)\".format(i+1,score))\n",
        "  df_grid.loc[gsr, 'auc'] = np.asarray(auc_score).mean()\n",
        "  df_score=df_score.append(df_grid.loc[gsr])\n",
        "  df_score.to_csv(\"/content/drive/MyDrive/ptb-xl-gridsearch/gridsearch_score_all.csv\", index=False)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvnNlyCX5oSi"
      },
      "source": [
        "# Evaluate model on validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1rgPocD5oSi",
        "outputId": "6d91d6ab-7d00-4433-cdbd-2d7fd083cdaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "aggregating predictions...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>macro_auc</th>\n",
              "      <th>Fmax</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.931458</td>\n",
              "      <td>0.827961</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   macro_auc      Fmax\n",
              "0   0.931458  0.827961"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_val_pred = model.predict(X_val)\n",
        "utils.evaluate_experiment(y_val, y_val_pred)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.4 64-bit ('ecg_python37': conda)",
      "language": "python",
      "name": "python37464bitecgpython37condacca13046f89242bdbe235da55b7380ab"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}